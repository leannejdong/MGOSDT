{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fantastic-infrastructure",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $\\color{orange}{\\text{Multiway Split Sparse Decision Tree}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-packet",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<span style='color:orange'> \n",
    "\n",
    "* <a href=\"#Introduction\">Introduction</a> <br>\n",
    "* <a href=\"#Bit-Maths\">Bit Maths</a> <br>\n",
    "* <a href=\"#Design-and-analysis-of-Algorithms\">Design and analysis of Algorithms</a> <br>\n",
    "* <a href=\"#Implementation\">Implementation</a> <br>\n",
    " - <a href=\"#Refactoring\">Refactoring</a> <br>\n",
    " - <a href=\"#Unit-Tests\">Unit Tests</a> <br>\n",
    " - <a href=\"#Static-Analysis\">Static Analysis</a> <br>\n",
    " - <a href=\"#Dynamic-Code-Analysis\">Dynamic Code Analysis</a> <br>\n",
    " - <a href=\"#Benchmarking\">Benchmarking</a> <br>\n",
    "* <a href=\"#Embedding\">Embedding MGOSDT in Python</a> <br>\n",
    "* <a href=\"#Contribution\">Contribution</a> <br>\n",
    "* <a href=\"#Future\">Future work</a> <br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-proposition",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "<img src=\"tree0.png\" alt=\"drawing\" width=\"550\"/>\n",
    "Decision tree has been popular since the early days of Machine Learning due to its interpretibility. Historically, the way we construct it is simple. We start with some root, keep branching it out til we hit some kind of stopping condition, at which point we can optionally prune some of the branches to avoid overfitting. The problem with this very plain algorithm (top down, prune it afterward) is the lack of optimality, if we choose the wrong point at the very beginning, there is no way to undo it. There has been numerous attempts to fix this problem via Mathematical optimization solver (or neural networks). However, in order to fully optimize a decision tree, we need to go through a search space that is both in both theory and practice hard. It does combinotorially explosion in the number of subtrees one can consider. \n",
    "\n",
    "The goal of my work is to produce a optimal sparse, **multiway-split** decision tree.  We follow a specialized algorithm that combines *dynamic programming* and *branch and bound* to optimize a generalize objective:\n",
    "$$ \\min_{\\text{tree}} \\hat{L}(\\text{tree}, \\{(x_i, y_i)\\}_i) = l(\\text{tree}, \\{(x_i, y_i)\\}_i) + \\lambda(\\#\\text{leaves in tree}).$$\n",
    "We minimize the misclassification error over all possible n-ary trees and sparsity. Here $\\lambda$ is the trade-off parameter that govern the predictive performance and sparsity. (or in brief, regularizing the number of leaves).\n",
    "\n",
    "We remark that this algorithm solve the problem of Optimality (NP hard) by leveraging computational caching. No greedy splitting and pruning like C4.5 and CART. When we could solve the problem of optimality, we get sparse accurate tree. Our approach has several important insights\n",
    "\n",
    "1. **Analytical Bounds**: The collection of bounds show that some partial trees can never be extended to form optimal tree, thus reducing the search space, without sacrificing the optimality of our algorithm)\n",
    "\n",
    "2. Dynamic Programming and **dependency graph** (See pptx)\n",
    "\n",
    "* Start with some datasets, apply some naive labels\n",
    "* Split into subsets using each feature\n",
    "* Keep splitting until higher accuracy is attained\n",
    "* Consolidate any duplication is found. (Solution for one duplicated instance can be used as the solution by another instance.\n",
    "* The DP formulation creates a dependency graph between sets and subsets. Each set is responsible for finding optimal features to sub-divide itself into additional subsets. Then each subset decide the best fit to split. Once enough subsets are decided, we collapse the trees,  the optimal tree emerge as a Directed Acyclic graph of best features.\n",
    "\n",
    "3. Tree representation by its leaves (Store bounds and intermediate results within each leave) ![tree representation](leaves.png)\n",
    "\n",
    "4. **Permutation map**: Discover identical trees already evaluated.\n",
    "5. **Leaf-based representation**: We store a bit vector, indicating which data point has features corresponding to the features described by its leaves (**Bit masking**. See `bitmask.cpp`, `dataset.cpp` and `encoder.cpp`)\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <cassert>\n",
    "#include <vector>\n",
    "// The purpose of the bit function is to tell us whether a certain bit of an int is set to 1\n",
    "// By doing &1 we are getting a value where all bits other than bit 0 are 0\n",
    "// The bit function does\n",
    "// 1 move the desired bit to bit 0 (by shifting)\n",
    "// 2 set all bits other than bit 0 to 0 (by adding)\n",
    "// 3 result true if the result is not 0 (by converting to bool)\n",
    "static bool bit(int index, int value)\n",
    "{\n",
    "   return value & (1 << index); \n",
    "}\n",
    "\n",
    "#include <vector>\n",
    "std::vector<bool> makeBitVector(size_t rain, bool construction, bool rush_hour, bool friday)\n",
    "{\n",
    "  std::vector<bool> result(5, false);\n",
    "  result[0] = bit(0, rain);// result[0] is the first bit of rain (bit 0)\n",
    "  result[1] = bit(1, rain);// result[1] is the 2nd bit of rain (bit 1)\n",
    "  result[2] = construction;\n",
    "  result[3] = rush_hour;\n",
    "  result[4] = friday;\n",
    "  return result;\n",
    "}\n",
    "static bool bit(int index, int value)\n",
    "{\n",
    "   return value & (1 << index); // check if index^th bit is present in the subset\n",
    "}\n",
    "```\n",
    "[bitmasking via vector of bool](https://godbolt.org/z/qYnxE4rzx)\n",
    "\n",
    "[bitmasking via bitset](https://godbolt.org/z/WPbf7GKhc)\n",
    "[bitmasking multiple bits](https://godbolt.org/z/456ovEcx8)\n",
    "\n",
    "6. Caching of intermediate results make our computations very fast. (Since cache memory is the fastest type of memory in your computer)\n",
    "\n",
    "Question: Does each set correspond to a task? or do the sets and subsets have nothing to do with the arrangement of work?\n",
    "\n",
    "In my [codes](https://gitlab.com/leannejdong/mgosdt/-/blob/main/src/optimizer/extraction/models.hpp)\n",
    "the function Line25 creates a set of optimal trees. It is essentially the dependency graph in the context of concurrency programming. Indeed, tree, subtree; problem, sub-problem; set, subset in mgosdt are the `task`, which are the problems entered by the bit vectors (from the bitmask class).\n",
    "\n",
    "7. Incremental computation\n",
    "The bounds of bit vector in each leave also let us to use incremental computations to evaluate the children of the leaves- should we decide to split next or not...\n",
    "5. Multiway split\n",
    "\n",
    "* Give a more natural way to handle multivalued categorical features than binary splits. Even if we have 3 categories, a 2-way split still seems less interpretable\n",
    "\n",
    "* Give a more interpretable DT algorithm that can handle a larger variety of datasets.\n",
    "\n",
    "* m-way tree works much nicer on GPUs than binary trees. The shallowness usually help in the sense that we could compress it and save memory storing information. For instance,  B-tree, a kind of balanced search tree that is optimized for external memory. This is due to its shallow structure, so the height of a B tree is minimized, meaning that we don't have to access all the items everytime we call the method recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-leadership",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary search tree vs Binary decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-interstate",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A **binary search tree** is an efficient data structure for storing information which you’d like to look up later. For example, you can store n integers and look one up in log n time. At each node in the binary search tree you are simply asking whether the number you’re looking for is higher or lower (hence binary), until eventually you find what you’re looking for (think binary search).\n",
    "\n",
    "A **binary decision tree**, at least in the context of machine learning, is a function that maps an input space of data to an output space of classes. For example, say you want classify whether a product will get sold out on Black Friday or not. The input space is all the information about the product such as historical sales, product type, amount discounted, number in stock, etc. The output space has two classes: will sell out, or won’t sell out.\n",
    "\n",
    "Each node in the binary decision tree asks a binary question about the data, e.g does the product have a high discount?, does the product usually sell well?. Based on the answer to each question you will take the left or right branch to the next node, following it down to the bottom (a leaf node) where you will find your final answer about whether it will sell out or not.\n",
    "\n",
    "More generally, you can think of a binary decision tree as a decision making tool. It asks you a series of questions and gives you a decision based on your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-douglas",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples of multiway-split trees \n",
    "\n",
    "B* trees, suffix trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-garlic",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Represent each subproblem by bit vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-rover",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```cpp\n",
    "// a program converts integers to their bit representation \n",
    "#include <iostream>\n",
    "#include <bitset>\n",
    "int main(){\n",
    "    for(unsigned i; std::cin>>i;)\n",
    "        std::cout << std::dec << i << \"==\"\n",
    "                  << std::hex << \"0x\" << i << \"==\"\n",
    "                  << std::bitset<8*sizeof(unsigned)>{i}<<\"\\n\";//  #bits in an unsigned int\n",
    "// unsigned types work better with bit operations since we typically don't care about \n",
    "// negative numbers when dealing with bitwise operations. \n",
    "// Using an unsigned type gives us one more bit to work with\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-pathology",
   "metadata": {},
   "source": [
    "## Bit Maths\n",
    "Information gets stored in computer in term of binary numbers.\n",
    "\n",
    "**Bitmasks** are a very useful way to compress multiple boolean flags in a single variable. It can reduce memory usage and operations on bits are basically as fast as they can get. In practice, any time you want to have multiple flags describing something in your application, a bitmask could be the right tool for the job. \n",
    "\n",
    "Mask in Bitmask means hiding something. And Bitmask is nothing but a binary number that represents something. Take an example. Consider the set $A = \\{1, 2, 3, 4, 5\\}$. How do we represent the subset $\\{2, 4\\}$ using a bitmask of length 5? (**Answer**: the bit mask 01010 represents the subset $\\{2, 4\\}$). The benefits of using bitmask are\n",
    "\n",
    "* Set the $i^{th}$ bit using bitwise or: $b|(1<< i)$. Take $i = 0$.\n",
    "* Unset(clear) the $i^{th}$ bit: $b\\&!(1<< i)$. Take $i = 1$.\n",
    "* Set the $i^{th}$ bit: $b\\&(1<< i)$. Take $i = 3$. Then\n",
    "$$(1<<i) = 01000$$\n",
    "$$01010\\& 01000 = 01000$$\n",
    "<font color = orange>Exercise</font>: Given a set, count how many subsets have sum of elements greater than or equal to a given value. [subset sum](https://godbolt.org/z/1YdcrTdoK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-arrival",
   "metadata": {},
   "source": [
    "We start by creating a class called Bitmask. This class will handle all of our bit manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amended-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bitmask\n",
    "{    \n",
    "    public:\n",
    "        Bitmask();\n",
    "\n",
    "        // Overwrites this bitmask.\n",
    "        void SetMask(Bitmask& other); \n",
    "\n",
    "        // Returns binary representation of bitmask.\n",
    "        uint32_t GetMask() const; \n",
    "\n",
    "        // Returns true if bit at pos = 1, else false.\n",
    "        bool GetBit(int pos) const; \n",
    "\n",
    "        // Sets bit at specified pos to 1 or 0 (true or false).\n",
    "        void SetBit(int pos, bool on);\n",
    "\n",
    "        // Sets bit at pos to 1.\n",
    "        void SetBit(int pos); \n",
    "\n",
    "        // Sets bit at pos to 0.\n",
    "        void ClearBit(int pos);\n",
    "\n",
    "        // Sets all bits to 0.\n",
    "        void Clear(); \n",
    "\n",
    "    private:\n",
    "        uint32_t bits; // 1.\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-grant",
   "metadata": {},
   "source": [
    "```cpp\n",
    "Bitmask::Bitmask() : bits(0) { }\n",
    "\n",
    "void Bitmask::SetMask(Bitmask& other)\n",
    "{\n",
    "    bits = other.GetMask();\n",
    "}\n",
    "\n",
    "uint32_t Bitmask::GetMask() const\n",
    "{\n",
    "    return bits;\n",
    "}\n",
    "\n",
    "bool Bitmask::GetBit(int pos) const\n",
    "{\n",
    "    return (bits & (1 << pos)) != 0; // 1\n",
    "}\n",
    "\n",
    "// A simple helper method that calls set or clear bit\n",
    "void Bitmask::SetBit(int pos, bool on)\n",
    "{\n",
    "    if(on)\n",
    "    {\n",
    "        SetBit(pos);\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        ClearBit(pos);\n",
    "    }\n",
    "}\n",
    "\n",
    "void Bitmask::SetBit(int pos)\n",
    "{\n",
    "    bits = bits | 1 << pos; // 2\n",
    "}\n",
    "\n",
    "void Bitmask::ClearBit(int pos)\n",
    "{\n",
    "    bits = bits & ~(1 << pos); // 3\n",
    "}\n",
    "\n",
    "void Bitmask::Clear()\n",
    "{\n",
    "    bits = 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-jesus",
   "metadata": {},
   "source": [
    "In MGOSDT, We define a function module as a collection of functions. \n",
    "* This declaration acts as both a function module and a container class. \n",
    "\n",
    "* The static class methods implements a function module providing operations on arrays of type bitblock, which can be allocated on the stack.\n",
    "\n",
    "* The non-static class methods implements a heap-allocated equivalence, which supports the same methods @note: Many of the binary operations assume that operands have the same length\n",
    "\n",
    "[bitmask.hpp](https://gitlab.com/leannejdong/mgosdt/-/blob/dev1/src/bitmask.hpp)\n",
    "[bitmask.cpp](https://gitlab.com/leannejdong/mgosdt/-/blob/dev1/src/bitmask.cpp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-rally",
   "metadata": {},
   "source": [
    "## Design and analysis of Algorithms\n",
    "\n",
    "### The DPB algorithm\n",
    "\n",
    "* Operates on weighted, additive, non-negative loss functions.\n",
    "\n",
    "$$ \\min_{\\text{tree}} \\hat{L}(\\text{tree}, \\{(x_i, y_i)\\}_i) = l(\\text{tree}, \\{(x_i, y_i)\\}_i) + \\lambda(\\#\\text{leaves in tree}).$$\n",
    "\n",
    "* **DP** allows us to decompose a problem into smaller child problems that can be solved recursively through a function call.\n",
    "\n",
    "* The **parallelism** allows us to solve the sub problems in parallel by delegating work to a separate thread.\n",
    "\n",
    "* The **branch and bound** algorithm let us to prune the search space. In the case of mgosdt, the analytical bounds allows us to eliminate some part of the search space.\n",
    "How does BB help? If we know the optimal cost is less than X, and we know the current branch of our search space cost more than X, then we can disregard this branch, since we know it won't lead to the optimal cost.\n",
    "\n",
    "### The Primary Data structure:\n",
    "\n",
    "GOSDT mains two primary data structures:\n",
    "\n",
    "* a concurrent *priority queue* is used to schedule problems to solve. Recall, a priority queue maintains an ordering in the queue based on the priorities of individual queue items. A normal queue has a FIFO policy, whereas a priorrity queue sorts its items. The C++ stl implementation `std::priority_queue` is not thread-safe. The *concurrent* usage means that the values from all three can change due to push/pop methods in other threads. The `tbb::concurrent_priority_queue` support `std::priority_queue`'s methods of size, empty and swap and is **thread-safe**.\n",
    "\n",
    "* a dependency graph is used to store problems and their dependency relationships. The dependency relationship $dep(p_{\\pi}, p_c)$ is defined between problems $p_{\\pi}$ and $p_c$ only if the solution of $p_{\\pi}$ depends on the solution of $p_c$. Each $p_c$ is further specified as $p^j_l$ or $p^j_r$ indicating that it is the left and right branch produced by splitting on feature $j$.\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "* Algorithm 1\n",
    "![GOSDT](GOSDT.png)\n",
    "* Algorithm 2 get_lower_bound$(s, Z, z^-, z^+)->lb)$\n",
    "* Algorithm 3 get_upper_bound$(s, Z, z^-, z^+)->ub)$\n",
    "* Algorithm 4 fails_bound$(p)->v$\n",
    "* Algorithm 5 $\\text{split}(s, j, Z)->s_l, s_r$\n",
    "![split](split.png)\n",
    "* Algorithm 6 an extraction algorithm that is used to construct the optimal tree from the dependency graph once the main GOSDT algorithm completes.\n",
    "![extract](extract.png)\n",
    "\n",
    "\n",
    "In my [codes](https://gitlab.com/leannejdong/mgosdt/-/blob/main/src/optimizer/extraction/models.hpp)\n",
    "the function Line25 creates a set of optimal trees. It is essentially the dependency graph in the context of concurrency programming. Indeed, tree, subtree; problem, sub-problem; set, subset in mgosdt are the `task`, which are the problems entered by the bit vectors (from the bitmask class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-scholarship",
   "metadata": {},
   "source": [
    "## Dynamic programming formulation\n",
    "\n",
    "* Each dataset presents a classfication problem.\n",
    "* When we alter the data by condition, the resulting subset presents a new problem.\n",
    "* Each filtering creates a new subproblem.\n",
    "* If two sets of conditions result in the same subset, then the solution of that subset can be used for both sets of conditions.\n",
    "\n",
    "This allows us to reduce the amount of computations to reach the solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-karaoke",
   "metadata": {},
   "source": [
    "## Contribution\n",
    "\n",
    "Our work presents\n",
    "\n",
    "* A new algorithm convert binary decision tree to n-ary decision tree.\n",
    "* Efficient memory management leads to a speed up! Prevent memory allocation does generally speed up a program.\n",
    "* Performance ptimization. \n",
    " - Completed profiling that allows us to detect which part of mgosdt are slow.\n",
    " - We came up with ideas about what speed up mgosdt\n",
    " - We experimented and performed benchmarking which validated things actually work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-latitude",
   "metadata": {},
   "source": [
    "## Some common questions on multiway vs Binary decision tree?\n",
    "\n",
    "Given the tree's structure, any one of the `N` attributes can be encoded in $log_2(N)` bits.\n",
    "* Do n-ary DT lead a run time improvement?\n",
    "\n",
    "No. There is no difference in term of complexity (both time and space). Every node may have multiple children (over 2) but the running time is still $O(\\log N)$.\n",
    "$$\\log_n N = \\frac{\\log_a N}{\\log_a b}.$$\n",
    "Here, the $\\log_a b$ is just a constant so it does not matter. So we can change the base of the logarithm and the running time complexity for the algorithm stay the same.\n",
    "$$ O(c*\\log N) = c O(\\log N) = O(\\log N) .$$ This is why, the branching factor (i.e. the number of children a node may have) does not matter.\n",
    "* Do n-ary DT gives better Interpretability?\n",
    "\n",
    "Yes. For the reasons mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-prince",
   "metadata": {},
   "source": [
    "## Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-jordan",
   "metadata": {},
   "source": [
    "### Refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-reflection",
   "metadata": {},
   "source": [
    "Static and dynamic code analyses are performed during source code reviews of gosdt and mgosdt. Static code analysis is done during the development of our codebase; Then we perform dynamic code analysis in studying how the code behaves during execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-injection",
   "metadata": {},
   "source": [
    "### Static Code Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-success",
   "metadata": {},
   "source": [
    "### Dynamic Code Analysis \n",
    "\n",
    "- **Thread, address sanitizer**. We debug our codes with  address sanitizer with `fsanitize` flag. There were large number of memory leaks, data races and unitialized memory issues on the original GOSDT codebase.\n",
    "- Valgrind\n",
    "- **CPU Profiler**. We run Linux profiler `perf` combine with flame chart to get a better overview of possible red spots might cause performance slow down. Nothing been detected. See [cpu-profiling](https://gitlab.com/leannejdong/mgosdt/-/blob/main/cpu-profiling.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accomplished-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import xgboost as xgb\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-truck",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.2\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-musician",
   "metadata": {},
   "source": [
    "Let’s get all of our data set up. We’ll start off by creating a train-test split so we can see just how well XGBoost performs. We’ll go with an 80%-20% split this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fatal-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greek-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order for XGBoost to be able to use our data, we’ll need to transform it into a specific format that XGBoost can handle. That format is called DMatrix. \n",
    "#It’s a very simple one-linear to transform a numpy array of data to DMatrix format:\n",
    "\n",
    "D_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "forty-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define xgboost\n",
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acoustic-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:18:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#Training and Testing\n",
    "#We can finally train our model similar to how we do so with Scikit Learn:\n",
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "progressive-wheel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8444444444444444\n",
      "Recall = 0.8055555555555555\n",
      "Accuracy = 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "#Let’s now run an evaluation. \n",
    "# Again the process is very similar to that of training models in Scikit Learn:\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(Y_test, best_preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-fortune",
   "metadata": {},
   "source": [
    "### Further Exploration with XGBoost\n",
    "That just about sums up the basics of XGBoost. But there are some more cool features that’ll help you get the most out of your models.\n",
    "The gamma parameter can also help with controlling overfitting. It specifies the minimum reduction in the loss required to make a further partition on a leaf node of the tree. I.e if creating a new node doesn’t reduce the loss by a certain amount, then we won’t create it at all.\n",
    "The booster parameter allows you to set the type of model you will use when building the ensemble. The default is gbtree which builds an ensemble of decision trees. If your data isn’t too complicated, you can go with the faster and simpler gblinear option which builds an ensemble of linear models.\n",
    "Setting the optimal hyperparameters of any ML model can be a challenge. So why not let Scikit Learn do it for you? We can combine Scikit Learn’s grid search with an XGBoost classifier quite easily:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "parameters = {\n",
    "     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "     }\n",
    "\n",
    "grid = GridSearchCV(clf,\n",
    "                    parameters, n_jobs=4,\n",
    "                    scoring=\"neg_log_loss\", cv=3)\n",
    "grid.fit(X_train, Y_train)\n",
    "model.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-court",
   "metadata": {},
   "source": [
    "## Embedding MGOSDT in Python\n",
    "\n",
    "Our goal is to being able to run [example.py](https://gitlab.com/leannejdong/mgosdt/-/blob/main/python/example.py)\n",
    "which calls the `GOSDT` class in [gosdt.py](https://gitlab.com/leannejdong/mgosdt/-/blob/main/python/model/gosdt.py).\n",
    "This GOSDT class uses the C++ extension module `gosdt` defined in `python_extension.cpp`.\n",
    "The implementations are inherantly done in C++ by creating a `GOSDT` object called `model`, which perform operations `fit`,  `predict`, `score`.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model.gosdt import GOSDT\n",
    "\n",
    "dataframe = pd.DataFrame(pd.read_csv(\"/home/leanne/Dev/mgosdt/experiments/datasets/iris/data.csv\"))\n",
    "\n",
    "X = dataframe[dataframe.columns[:-1]]\n",
    "y = dataframe[dataframe.columns[-1:]]\n",
    "\n",
    "hyperparameters = {\n",
    "    \"regularization\": 0.04,\n",
    "    \"time_limit\": 3600,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "model = GOSDT(hyperparameters)\n",
    "model.fit(X, y)\n",
    "# model.load(\"python/model/model.json\")\n",
    "# model.load(\"../gosdt_icml/model.json\")\n",
    "print(\"Execution Time: {}\".format(model.time))\n",
    "\n",
    "prediction = model.predict(X)\n",
    "training_accuracy = model.score(X, y)\n",
    "print(\"Training Accuracy: {}\".format(training_accuracy))\n",
    "print(\"Size: {}\".format(model.leaves()))\n",
    "print(\"Loss: {}\".format(1 - training_accuracy))\n",
    "print(\"Risk: {}\".format(\n",
    "    model.leaves() * hyperparameters[\"regularization\"]\n",
    "    + 1 - training_accuracy))\n",
    "model.tree.__initialize_training_loss__(X, y)\n",
    "print(model.tree)\n",
    "print(model.latex())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-yesterday",
   "metadata": {},
   "source": [
    "It is simple C! First we need a function take some python object as input. We need to parse them and get some native variables in C++. In this case, we struct some char pointer that store the python objects. Then we do our operations in C++, configuring algorithm. Then do the step back from my native variables to python objects. Next, register the function within a module's symbol table. It is a table in which declared which functions are being supported by the module. (Remember, all Python functions live in a module, even if they are actually C functions) The name `add` would be the python name we use to call this module.\n",
    "Finally, we have to declare the initialization of the module. It is the function will be called when importing the module\n",
    "### Python C API\n",
    "```cpp\n",
    "#include <Python.h>\n",
    "#include <iostream>\n",
    "#include <fstream>\n",
    "#include <sstream>\n",
    "#include <string>\n",
    "#include \"gosdt.hpp\"\n",
    "// @param args: contains a single string object which is a JSON string containing the algorithm configuration\n",
    "static PyObject * configure(PyObject * self, PyObject * args) {\n",
    "    const char * configuration;\n",
    "    if (!PyArg_ParseTuple(args, \"s\", & configuration)) { return NULL; }\n",
    "\n",
    "    std::istringstream config_stream(configuration);\n",
    "    GOSDT::configure(config_stream);\n",
    "\n",
    "    return Py_BuildValue(\"\");\n",
    "}\n",
    "\n",
    "// @param args: contains a single string object which contains the training data in CSV form\n",
    "// @returns a string object containing a JSON array of all resulting models\n",
    "static PyObject * fit(PyObject * self, PyObject * args) {\n",
    "    const char * dataset;\n",
    "    if (!PyArg_ParseTuple(args, \"s\", & dataset)) { return NULL; }\n",
    "\n",
    "    std::istringstream data_stream(dataset);\n",
    "    GOSDT model;\n",
    "    std::string result;\n",
    "    model.fit(data_stream, result);\n",
    "\n",
    "    return Py_BuildValue(\"s\", result.c_str());\n",
    "}\n",
    "\n",
    "// @returns the number of seconds spent training\n",
    "static PyObject * time(PyObject * self, PyObject * args) { return Py_BuildValue(\"f\", GOSDT::time); }\n",
    "\n",
    "// @returns the number of iterations spent training\n",
    "static PyObject * iterations(PyObject * self, PyObject * args) { return Py_BuildValue(\"i\", GOSDT::iterations); }\n",
    "\n",
    "// @returns the number of vertices in the depency graph\n",
    "static PyObject * size(PyObject * self, PyObject * args) { return Py_BuildValue(\"i\", GOSDT::size); }\n",
    "\n",
    "// @returns the current status code\n",
    "static PyObject * status(PyObject * self, PyObject * args) { return Py_BuildValue(\"i\", GOSDT::status); }\n",
    "\n",
    "// Define the list of methods Python intepreter needs to know about for a module\n",
    "static PyMethodDef gosdt_methods[] = {\n",
    "    // { method name, method pointer, method parameter format, method description }\n",
    "    {\"configure\", configure, METH_VARARGS, \"Configures the algorithm using an input JSON string\"},\n",
    "    {\"fit\", fit, METH_VARARGS, \"Trains the model using an input CSV string\"},\n",
    "    {\"time\", time, METH_NOARGS, \"Number of seconds spent training\"},\n",
    "    {\"iterations\", iterations, METH_NOARGS, \"Number of iterations spent training\"},\n",
    "    {\"size\", size, METH_NOARGS, \"Number of vertices in the depency graph\"},\n",
    "    {\"status\", status, METH_NOARGS, \"Check the status code of the algorithm\"},\n",
    "    {NULL, NULL, 0, NULL}\n",
    "};\n",
    "\n",
    "// Define the module\n",
    "static struct PyModuleDef gosdt = {\n",
    "    PyModuleDef_HEAD_INIT,\n",
    "    \"gosdt\", // Module Name\n",
    "    \"Generalized Optimal Sparse Decision Tree\", // Module Description\n",
    "    -1, // Size of per-interpreter state\n",
    "    gosdt_methods // Module methods\n",
    "};\n",
    "\n",
    "// Initialize the module\n",
    "PyMODINIT_FUNC PyInit_gosdt(void) {\n",
    "    return PyModule_Create(&gosdt);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-constraint",
   "metadata": {},
   "source": [
    "## Importing C++ extension to the Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "animated-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leanne\n"
     ]
    }
   ],
   "source": [
    "%cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "composite-elements",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leanne/Dev/mgosdt\n"
     ]
    }
   ],
   "source": [
    "%cd Dev/mgosdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "international-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gosdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "numerous-ideal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leanne/Dev/mgosdt/python\n"
     ]
    }
   ],
   "source": [
    "%cd ../python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "professional-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model.gosdt import GOSDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "decent-project",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {\n",
      "    \"uncertainty_tolerance\": 0.0,\n",
      "    \"regularization\": 0.01,\n",
      "    \"upperbound\": 0.0,\n",
      "    \n",
      "    \"time_limit\": 86400,\n",
      "    \"worker_limit\": 1,\n",
      "    \"tile_limit\": 1,\n",
      "    \"model_limit\": 1,\n",
      "\n",
      "    \"verbose\": true,\n",
      "    \"diagnostics\": false,\n",
      "    \"balance\": true,\n",
      "    \"non_binary\": true,\n",
      "\n",
      "    \"look_ahead\": true,\n",
      "    \"similar_support\": false,\n",
      "    \"greedy_sampling\": false,\n",
      "    \"cancellation\": true,\n",
      "    \"indifference\": false,\n",
      "    \"strong_indifference\": false\n",
      "}\n",
      "\n",
      "Data: outlook,temp,humidity,wind,play\n",
      "Sunny,Hot,High,Weak,No\n",
      "Sunny,Hot,High,Strong,No\n",
      "Overcast,Hot,High,Weak,Yes\n",
      "Rain,Mild,High,Weak,Yes\n",
      "Rain,Cool,Normal,Weak,Yes\n",
      "Rain,Cool,Normal,Strong,No\n",
      "Overcast,Cool,Normal,Strong,Yes\n",
      "Sunny,Mild,High,Weak,No\n",
      "Sunny,Cool,Normal,Weak,Yes\n",
      "Rain,Mild,Normal,Weak,Yes\n",
      "Sunny,Mild,Normal,Strong,Yes\n",
      "Overcast,Mild,High,Strong,Yes\n",
      "Overcast,Hot,Normal,Weak,Yes\n",
      "Rain,Mild,High,Strong,No\n",
      "\n",
      "Result:  [\n",
      "  {\n",
      "    \"children\": [\n",
      "      {\n",
      "        \"in\": \"Overcast\",\n",
      "        \"then\": {\n",
      "          \"complexity\": 0.009999999776482582,\n",
      "          \"loss\": 0.0,\n",
      "          \"name\": \"play\",\n",
      "          \"prediction\": \"Yes\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"in\": \"Rain\",\n",
      "        \"then\": {\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"in\": \"Weak\",\n",
      "              \"then\": {\n",
      "                \"complexity\": 0.009999999776482582,\n",
      "                \"loss\": 0.0,\n",
      "                \"name\": \"play\",\n",
      "                \"prediction\": \"Yes\"\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"in\": \"default\",\n",
      "              \"then\": {\n",
      "                \"complexity\": 0.009999999776482582,\n",
      "                \"loss\": 0.0,\n",
      "                \"name\": \"play\",\n",
      "                \"prediction\": \"No\"\n",
      "              }\n",
      "            }\n",
      "          ],\n",
      "          \"feature\": 3,\n",
      "          \"name\": \"wind\",\n",
      "          \"type\": \"categorical\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"in\": \"default\",\n",
      "        \"then\": {\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"in\": \"Normal\",\n",
      "              \"then\": {\n",
      "                \"complexity\": 0.009999999776482582,\n",
      "                \"loss\": 0.0,\n",
      "                \"name\": \"play\",\n",
      "                \"prediction\": \"Yes\"\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"in\": \"default\",\n",
      "              \"then\": {\n",
      "                \"complexity\": 0.009999999776482582,\n",
      "                \"loss\": 0.0,\n",
      "                \"name\": \"play\",\n",
      "                \"prediction\": \"No\"\n",
      "              }\n",
      "            }\n",
      "          ],\n",
      "          \"feature\": 2,\n",
      "          \"name\": \"humidity\",\n",
      "          \"type\": \"categorical\"\n",
      "        }\n",
      "      }\n",
      "    ],\n",
      "    \"feature\": 0,\n",
      "    \"name\": \"outlook\",\n",
      "    \"type\": \"categorical\"\n",
      "  }\n",
      "]\n",
      "Time (seconds):  0.003000000026077032\n",
      "Iterations:  349\n",
      "Graph Size:  131\n"
     ]
    }
   ],
   "source": [
    "import gosdt\n",
    "\n",
    "with open (\"/home/leanne/Dev/mgosdt/experiments/datasets/tennis/tennis.csv\", \"r\") as data_file:\n",
    "    data = data_file.read()\n",
    "\n",
    "with open (\"/home/leanne/Dev/mgosdt/experiments/configurations/debug.json\", \"r\") as config_file:\n",
    "    config = config_file.read()\n",
    "\n",
    "\n",
    "print(\"Config:\", config)\n",
    "print(\"Data:\", data)\n",
    "\n",
    "gosdt.configure(config)\n",
    "result = gosdt.fit(data)\n",
    "\n",
    "print(\"Result: \", result)\n",
    "print(\"Time (seconds): \", gosdt.time())\n",
    "print(\"Iterations: \", gosdt.iterations())\n",
    "print(\"Graph Size: \", gosdt.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "connected-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 11.60099983215332\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(pd.read_csv(\"/home/leanne/Dev/mgosdt/experiments/datasets/iris/data.csv\"))\n",
    "\n",
    "X = dataframe[dataframe.columns[:-1]]\n",
    "y = dataframe[dataframe.columns[-1:]]\n",
    "\n",
    "hyperparameters = {\n",
    "    \"regularization\": 0.04,\n",
    "    \"time_limit\": 3600,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "model = GOSDT(hyperparameters)\n",
    "model.fit(X, y)\n",
    "print(\"Execution Time: {}\".format(model.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "split-robinson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 0.0\n"
     ]
    }
   ],
   "source": [
    "#dataframe = pd.DataFrame(pd.read_csv(\"/home/leanne/Dev/mgosdt/experiments/datasets/monk_2/data.csv\"))\n",
    "dataframe = pd.DataFrame(pd.read_csv(\"/home/leanne/Dev/mgosdt/experiments/datasets/tennis/tennis.csv\"))\n",
    "X = dataframe[dataframe.columns[:-1]]\n",
    "y = dataframe[dataframe.columns[-1:]]\n",
    "\n",
    "hyperparameters = {\n",
    "    \"regularization\": 0.1,\n",
    "    \"time_limit\": 3600,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "model = GOSDT(hyperparameters)\n",
    "model.fit(X, y)\n",
    "print(\"Execution Time: {}\".format(model.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-virgin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
